{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb26df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import figure\n",
    "from matplotlib.backends import backend_agg\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability.python.internal import tf_keras\n",
    "from safetensors.tensorflow import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216369d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_v2_behavior\n",
    "\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "sys.argv = sys.argv[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import seaborn as sns\n",
    "    HAS_SEABORN = True\n",
    "except ImportError:\n",
    "    HAS_SEABORN = False\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "IMAGE_SHAPE = [28, 28, 1]\n",
    "NUM_TRAIN_EXAMPLES = 60000\n",
    "NUM_HELDOUT_EXAMPLES = 10000\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "flags.DEFINE_float('learning_rate',\n",
    "                   default = 0.001,\n",
    "                   help = 'Initital learning rate.')\n",
    "\n",
    "flags.DEFINE_integer('num_epochs',\n",
    "                     default = 2,\n",
    "                     help = 'Number of training steps to run.')\n",
    "\n",
    "flags.DEFINE_integer('batch_size',\n",
    "                     default = 10,\n",
    "                     help = 'Batch size.')\n",
    "\n",
    "flags.DEFINE_string('data_dir',\n",
    "                    default=os.path.join(os.getenv('TEST_TMPDIR', 'tmp'),\n",
    "                                         'bayesian_neural_network/data'),\n",
    "                                         help='Directory where data is stored (if using real data).')\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'model_dir',\n",
    "    default=os.path.join(os.getenv('TEST_TMPDIR', 'tmp'),\n",
    "                         'bayesian neural network/'),\n",
    "    help=\"Directory to put the model's fit.\")\n",
    "\n",
    "flags.DEFINE_integer('viz_steps',\n",
    "                     default=400,\n",
    "                     help='Frequency at which save visualizations.')\n",
    "\n",
    "flags.DEFINE_integer('num_monte_carlo',\n",
    "                     default=50,\n",
    "                     help='Network draws to compute predictive probabilities.')\n",
    "\n",
    "flags.DEFINE_bool('fake_data',\n",
    "                  default=False,\n",
    "                  help='If true, use fake data. Defaults to real data.')\n",
    "\n",
    "flags.DEFINE_string('checkpoint_path', 'model_checkpoint.weights.h5', help= 'Name of the file to save model weights.')\n",
    "\n",
    "flags.DEFINE_string('final_path', 'final_model.keras', help= 'path storing the model.')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_posteriors(names, qm_vals, qs_vals, fname):\n",
    "    \"\"\"Save a PNG plot with histograms of weight means and stddevs.\n",
    "    \n",
    "    Args:\n",
    "        names: A python `iterable` of `str` variable names.\n",
    "          qm_vals: A python `iterable`, the same length as `names`,\n",
    "            whose elements are Numpy `array`s, of any shape, containing\n",
    "            posterior means of weight variables.\n",
    "          qs_vals: A python `iterable`, the same length as `names`,\n",
    "            whose elements are Numpy `array`s, of any shape, containing\n",
    "            posterior standard deviations of weight variables.\n",
    "        fname: Python `str` filename to save the plot to.\n",
    "        \n",
    "    \"\"\"\n",
    "    fig = figure.Figure(figsize=(6,3))\n",
    "    canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    for n, qm in zip(names, qm_vals):\n",
    "        sns.displot(qm.reshape([-1]), ax=ax, label=n)\n",
    "    ax.set_title('weight means')\n",
    "    ax.set_xlim([-1.5, 1.5])\n",
    "    ax.legend()\n",
    "\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    for n, qs in zip(names, qs_vals):\n",
    "        sns.displot(qs.reshape([-1]), ax=ax)\n",
    "    ax.set_title('weight stddevs')\n",
    "    ax.set_xlim([0, 1.])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    canvas.print_figure(fname, format='png')\n",
    "    print('saved {}'.format(fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb97945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heldout_prediction(input_vals, probs,\n",
    "                            fname, n=10, title=''):\n",
    "    \"\"\"Save a PNG plot visualizing posterior uncertainity heldout data.\n",
    "    \n",
    "    Args:\n",
    "        input_vals: A `float`-like Numpy `array` of shape\n",
    "          `[num_heldout] + IMAGE_SHAPE`, containing heldout input images.\n",
    "        probs: A `float`-line Numpy array of shape `[num_monte_carlo,\n",
    "          num_heldout, num_classes]` containing Monte Carlo samples of\n",
    "          class probabilities for each heldout sample.\n",
    "        fname: Python `str` filename to save the plot to.\n",
    "        n: Python `int` number of datapoints to visualize.\n",
    "        title: Python `str` title for the plot.\n",
    "    \"\"\"\n",
    "    fig = figure.Figure(figsize=(9, 3*n))\n",
    "    canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "    for i in range(n):\n",
    "        ax = fig.add_subplot(n, 3, 3*i +1)\n",
    "        ax.imshow(input_vals[i, :].reshape(IMAGE_SHAPE[:-1]), interpolation='None')\n",
    "\n",
    "        ax = fig.add_subplot(n, 3, 3*i +2)\n",
    "        for prob_sample in probs:\n",
    "            sns.barplot(x=np.arange(10), y=prob_sample[i, :], alpha=0.1, ax=ax)\n",
    "            ax.set_ylim([0,1])\n",
    "        ax.set_title('posterior samples')\n",
    "\n",
    "        ax = fig.add_subplot(n, 3, 3*i +3)\n",
    "        sns.barplot(x=np.arange(10), y=np.mean(probs[:, i, :], axis=0), ax=ax)\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.set_title('predictive probs')\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    canvas.print_figure(fname, format='png')\n",
    "    print('saved {}'.format(fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Creates a Keras model using LeNet-5 architecture.\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /\n",
    "                              tf.cast(NUM_TRAIN_EXAMPLES, dtype=tf.float32))\n",
    "    \n",
    "    model = tf_keras.models.Sequential([\n",
    "        tfp.layers.Convolution2DFlipout(\n",
    "            6, kernel_size=5, padding='SAME',\n",
    "            kernel_divergence_fn=kl_divergence_function,\n",
    "            activation=tf.nn.relu),\n",
    "        tf_keras.layers.MaxPooling2D(\n",
    "            pool_size=[2, 2], strides=[2,2],\n",
    "            padding='SAME'),\n",
    "        tfp.layers.Convolution2DFlipout(\n",
    "            16, kernel_size=5, padding='SAME',\n",
    "            kernel_divergence_fn=kl_divergence_function,\n",
    "            activation=tf.nn.relu),\n",
    "        tf_keras.layers.MaxPooling2D(\n",
    "            pool_size=[2, 2], strides=[2, 2],\n",
    "            padding='SAME'),\n",
    "        tfp.layers.Convolution2DFlipout(\n",
    "            120, kernel_size=5, padding='SAME',\n",
    "            kernel_divergence_fn=kl_divergence_function,\n",
    "            activation=tf.nn.relu),\n",
    "        tf_keras.layers.Flatten(),\n",
    "        tfp.layers.DenseFlipout(\n",
    "            84, kernel_divergence_fn=kl_divergence_function,\n",
    "            activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(\n",
    "            NUM_CLASSES, kernel_divergence_fn=kl_divergence_function,\n",
    "            activation=tf.nn.softmax)\n",
    "    ])\n",
    "    optimizer = tf_keras.optimizers.legacy.Adam(lr=FLAGS.learning_rate)\n",
    "    model.compile(optimizer, loss= 'categorical_crossentropy',\n",
    "                  metrics=['accuracy'], experimental_run_tf_function=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c887669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTSequence(tf_keras.utils.Sequence):\n",
    "    \"\"\"Produces a sequence of MNIST digits with labels.\"\"\"\n",
    "    def __init__(self, data=None, batch_size=128, fake_data_size=None):\n",
    "        \"\"\"Initializes the sequence.\n",
    "        \n",
    "        Args:\n",
    "          data: Tuple of numpy `array` instances, the first representing images and\n",
    "            the second labels.\n",
    "          batch_size: Integer, number of elements in each training batch.\n",
    "          fake_data_size: Optional integer number of fake datapoints to generate.\n",
    "        \"\"\"\n",
    "        if data:\n",
    "            images, labels = data\n",
    "        else:\n",
    "            images, labels = MNISTSequence.__generate_fake_data(\n",
    "                num_images=fake_data_size, num_classes=NUM_CLASSES)\n",
    "        self.images, self.labels = MNISTSequence.__preprocessing(\n",
    "            images, labels)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def __generate_fake_data(num_images, num_classes):\n",
    "        \"\"\"Generates fake data in the shape of the MNIST dataset for unittest.\n",
    "        \n",
    "        Args:\n",
    "          num_images: Integer, the number of fake images to be generated.\n",
    "          num_classes: Integer, the number of classes to be generated.\n",
    "        Returns:\n",
    "          images: Numpy `array` representing the fake image data. The\n",
    "            shape of the array will be (num_images, 28, 28).\n",
    "          labels: Numpy `array` of integers, where each entry will be\n",
    "            assigned a unique integer.\n",
    "        \"\"\"\n",
    "        images = np.random.randint(low=0, high=256,\n",
    "                                   size=(num_images, IMAGE_SHAPE[0],\n",
    "                                         IMAGE_SHAPE[1]))\n",
    "        labels = np.random.randint(low=0, high=num_classes,\n",
    "                                   size=num_images)\n",
    "        return images, labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def __preprocessing(images, labels):\n",
    "        \"\"\"Preprocesses image and labels data.\n",
    "        \n",
    "        Args:\n",
    "          images: Numpy `array` representing the image data.\n",
    "          labels: Numpy `array` representing the labels data (range 0-9).\n",
    "        \n",
    "        Returns:\n",
    "          images: Numpy `array` representing the image data, normalized\n",
    "            and expanded for convolutional network input.\n",
    "          labels: Numpy `array` representing the labels data (range 0-9),\n",
    "            as one-hot (categorical) values.\n",
    "        \"\"\"\n",
    "        images = 2 * (images / 255.) -1.\n",
    "        images = images[..., tf.newaxis]\n",
    "\n",
    "        labels = tf_keras.utils.to_categorical(labels)\n",
    "        return images, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(tf.math.ceil(len(self.images) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.images[idx * self.batch_size: (idx +1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size: (idx +1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88515d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Directory tmp/bayesian neural network/ already exists. Resuming if weights found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0221 07:40:32.777131 8434347072 839659448.py:12] Directory tmp/bayesian neural network/ already exists. Resuming if weights found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:>>> Resuming from last checkpoint: tmp/bayesian neural network/model_checkpoint.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0221 07:40:33.274456 8434347072 839659448.py:30] >>> Resuming from last checkpoint: tmp/bayesian neural network/model_checkpoint.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ... Training convolutional neural network\n",
      "INFO:tensorflow:final_model.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0221 07:40:33.363446 8434347072 839659448.py:91] final_model.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main(argv):\n",
    "        del argv\n",
    "\n",
    "        full_checkpoint_path = os.path.join(FLAGS.model_dir, FLAGS.checkpoint_path)\n",
    "\n",
    "        if not tf.io.gfile.exists(FLAGS.model_dir):\n",
    "            tf.compat.v1.logging.info(\n",
    "                'Creating new log directory at {}'.format(FLAGS.model_dir))\n",
    "            tf.io.gfile.makedirs(FLAGS.model_dir)\n",
    "        else:\n",
    "            tf.compat.v1.logging.info(\n",
    "                'Directory {} already exists. Resuming if weights found.'.format(FLAGS.model_dir)\n",
    "            )\n",
    "\n",
    "        if FLAGS.fake_data:\n",
    "            train_seq = MNISTSequence(batch_size=FLAGS.batch_size,\n",
    "                                      fake_data_size=NUM_TRAIN_EXAMPLES)\n",
    "            heldout_seq = MNISTSequence(batch_size = FLAGS.batch_size,\n",
    "                                        fake_data_size=NUM_HELDOUT_EXAMPLES)\n",
    "        else:\n",
    "            train_set, heldout_set = tf_keras.datasets.mnist.load_data()\n",
    "            train_seq = MNISTSequence(data=train_set, batch_size=FLAGS.batch_size)\n",
    "            heldout_seq = MNISTSequence(data=heldout_set, batch_size=FLAGS.batch_size)\n",
    "        \n",
    "        model = create_model()\n",
    "        model.build(input_shape=[None, 28,28, 1])\n",
    "\n",
    "        if tf.io.gfile.exists(full_checkpoint_path):\n",
    "            tf.compat.v1.logging.info('>>> Resuming from last checkpoint: {}'.format(full_checkpoint_path))\n",
    "            model.load_weights(full_checkpoint_path)\n",
    "\n",
    "        print(' ... Training convolutional neural network')\n",
    "        best_loss= float('inf')\n",
    "\n",
    "        for epoch in range(FLAGS.num_epochs):\n",
    "            epoch_accuracy, epoch_loss = [], []\n",
    "            # for step, (batch_x, batch_y) in enumerate(train_seq):\n",
    "            #     batch_loss, batch_accuracy = model.train_on_batch(\n",
    "            #         batch_x, batch_y)\n",
    "            #     epoch_accuracy.append(batch_accuracy)\n",
    "            #     epoch_loss.append(batch_loss)\n",
    "\n",
    "            #     if step % 100 == 0:\n",
    "            #       print('Epoch: {}, Batch index: {}, '\n",
    "            #      'Loss: {:.3f}, Accuracy: {:.3f}'.format(\n",
    "            #       epoch, step,\n",
    "            #       tf.reduce_mean(epoch_loss),\n",
    "            #       tf.reduce_mean(epoch_accuracy)))\n",
    "                  \n",
    "            #     if (step+1) % FLAGS.viz_steps == 0:\n",
    "            #         print(' ... Running monte carlo inference')\n",
    "            #         probs = tf.stack([model.predict(heldout_seq, verbose=1)\n",
    "            #                           for _ in range(FLAGS.num_monte_carlo)], axis = 0)\n",
    "            #         mean_probs = tf.reduce_mean(probs, axis=0)\n",
    "            #         heldout_log_prob = tf.reduce_mean(tf.math.log(mean_probs))\n",
    "            #         print(' ... Held-out nats: {:.3f}'.format(heldout_log_prob))\n",
    "\n",
    "            #         if HAS_SEABORN:\n",
    "            #             names = [layer.name for layer in model.layers\n",
    "            #                      if 'flipout' in layer.name]\n",
    "            #             qm_values = [layer.kernel_posterior.mean().numpy()\n",
    "            #                          for layer in model.layers\n",
    "            #                          if 'flipout' in layer.name]\n",
    "            #             qs_values = [layer.kernel_posterior.stddev().numpy()\n",
    "            #                         for layer in model.layers\n",
    "            #                         if 'flipout' in layer.name]\n",
    "                        \n",
    "            #             plot_weight_posteriors(names, qm_values, qs_values,\n",
    "            #                                    fname=os.path.join(\n",
    "            #                                        FLAGS.model_dir,\n",
    "            #                                        'epoch{}_step{:05d}_weights.png'.format(\n",
    "            #                                            epoch, step)\n",
    "            #                                    ))\n",
    "                        \n",
    "            #             plot_heldout_prediction(heldout_seq.images, probs.numpy(),\n",
    "            #                                     fname=os.path.join(\n",
    "            #                                         FLAGS.model_dir,\n",
    "            #                                         'epoch{}_step{}_pred.png'.format(\n",
    "            #                                             epoch, step)\n",
    "            #                                     ), title = 'mean heldout logprob {:.2f}'\n",
    "            #                                     .format(heldout_log_prob))\n",
    "        \n",
    "        current_epoch_loss = tf.reduce_mean(epoch_loss)\n",
    "        if current_epoch_loss < best_loss:\n",
    "            best_loss = current_epoch_loss\n",
    "            model.save_weights(full_checkpoint_path)\n",
    "            tf.compat.v1.logging.info('... Epoch {}: Model improved. Saved weights to {}'.format(epoch, full_checkpoint_path))\n",
    "\n",
    "        model.save(FLAGS.final_path)\n",
    "        tf.compat.v1.logging.info(FLAGS.final_path)\n",
    "        weights = {}\n",
    "        for layer in model.layers:\n",
    "            w = layer.get_weights()\n",
    "            \n",
    "            # CASE: Bayesian Kernel + Bayesian Bias (4 weights)\n",
    "            if len(w) == 4:\n",
    "                weights[f\"{layer.name}.kernel_mu\"] = tf.convert_to_tensor(w[0])\n",
    "                weights[f\"{layer.name}.kernel_rho\"] = tf.convert_to_tensor(w[1])\n",
    "                weights[f\"{layer.name}.bias_mu\"] = tf.convert_to_tensor(w[2])\n",
    "                weights[f\"{layer.name}.bias_rho\"] = tf.convert_to_tensor(w[3])\n",
    "                \n",
    "            # CASE: Bayesian Kernel + Deterministic Bias (3 weights)\n",
    "            elif len(w) == 3:\n",
    "                weights[f\"{layer.name}.kernel_mu\"] = tf.convert_to_tensor(w[0])\n",
    "                weights[f\"{layer.name}.kernel_rho\"] = tf.convert_to_tensor(w[1])\n",
    "                weights[f\"{layer.name}.bias\"] = tf.convert_to_tensor(w[2])\n",
    "                \n",
    "            # CASE: Standard Layer (2 weights: Kernel + Bias)\n",
    "            elif len(w) == 2:\n",
    "                weights[f\"{layer.name}.weight\"] = tf.convert_to_tensor(w[0])\n",
    "                weights[f\"{layer.name}.bias\"] = tf.convert_to_tensor(w[1])\n",
    "\n",
    "        save_file(weights, \"model.safetensors\")\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run(main)\n",
    "    except SystemExit:\n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbbd1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from safetensors.tensorflow import load_file\n",
    "# file_path = \"fixed_model.safetensors\"\n",
    "\n",
    "\n",
    "# import keras\n",
    "# from safetensors.tensorflow import save_file\n",
    "\n",
    "# # 1. Load your model\n",
    "# model = keras.models.load_model(\"final_model.keras\")\n",
    "\n",
    "# # 2. Extract weights as a dictionary of tensors\n",
    "# # We convert them to a dict that safetensors understands\n",
    "# weights_dict = {v.name: v.value() for v in model.weights}\n",
    "\n",
    "# # 3. Save to a new file\n",
    "# save_file(weights_dict, \"fixed_model.safetensors\")\n",
    "# print(\"Successfully converted .keras weights to fixed_model.safetensors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# try:\n",
    "#     # This returns a dictionary of {name: tf.Tensor}\n",
    "#     tensors = load_file(file_path)\n",
    "    \n",
    "#     print(f\"--- Found {len(tensors)} tensors in {file_path} ---\")\n",
    "    \n",
    "#     # Sort keys to make it easier to find the 'conv2d_flipout' group\n",
    "#     for name in sorted(tensors.keys()):\n",
    "#         tensor = tensors[name]\n",
    "#         print(f\"Name: {name:<50} | Shape: {tensor.shape}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error reading safetensors: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
